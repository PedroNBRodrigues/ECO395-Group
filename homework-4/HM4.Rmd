---
title: "Homework 4"
author: "Ahmed Almezail, Pedro Rodrigues and Sean Pierce"
date: "4/28/2022"
output: md_document
---
``` {r include=FALSE}
rm(list = ls())
library(tidyverse)
library(ggplot2)
library(LICORS)
library(foreach)
library(mosaic)
library(devtools)
library(ggfortify)
library(arules)  
library(arulesViz)
library(igraph)
library(splitstackshape)
```

# Question 1 - Clustering and PCA

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
wine <- read.csv("~/Desktop/Data-Mining/ECO395M-Mining/data/wine.csv")
```

First, let's have a look at the summary of the data. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
summary(wine)
```

##Let's start with clustring

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
X = wine[,-(12:13)]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
```

Now we create 2 clusters using K-means.

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
set.rseed(1)
clust1 = kmeans(X, 2, nstart=25)
clust1$center[1,]*sigma + mu
clust1$center[2,]*sigma + mu
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
qplot(color, quality, data=wine, color=factor(clust1$cluster)) +
  theme_minimal()
```
  
  
As we can notice, the algorithm succeeded in differentiating between the wine colors.  
  
  
Out of curiosity, let's have a look at the distribution of quality for each wine color.  
  
  
```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
ggplot(wine) +
 aes(x = quality) +
 geom_histogram(bins = 30L, fill = "#112446") +
 theme_minimal() +
 facet_wrap(vars(color))
```



White wines have higher frequency in quality that is higher than 7. That is relatively reflected in the clustered graph by having a point when quality = 9. However, we don't see that in the red whine. Also, there are no points for both wines when quality is lower than 3 which matches the distribution of the quality variable. So, we can say that the clustering satisfies the differentiating between red and white wines including their qualities. 

We have tried K = different values, but it failed to answer what is asked in the question. Therefore, we believe 2 clusters is an option that satisfies the this part.


However, let's see if can improve the in-sample fit by doing K-means++.



```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
set.rseed(1)
clust2 = kmeanspp(X, k=2, nstart=25)
clust2$center[1,]*sigma + mu
clust2$center[2,]*sigma + mu
```



```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
qplot(color, quality, data=wine, color=factor(clust2$cluster))
```



We got the same graph, but let's check the cluster sum of errors to double check.



```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
#Put them in a table 
Kmeans_withinss = clust1$tot.withinss
Kmeanspp_withinss = clust2$tot.withinss
Kmeans_betweenss  = clust1$betweenss
Kmeanspp_betweenss  = clust2$betweenss
kmeans_summary = data.frame(
Kmeans_withinss = Kmeans_withinss,
Kmeanspp_withinss = Kmeanspp_withinss,
Kmeans_betweenss = Kmeans_betweenss,
Kmeanspp_betweenss = Kmeanspp_betweenss)
kmeans_summary
```



There was no improvement when we did the K-means++. So the k-means itself was sufficient.



##Now let's work on PCA...




```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
wine.pca1 = prcomp(X, rank = 6)
summary(wine.pca1)
str(wine.pca1)
```



**From the results, we can notice that PC2 explains 50% of the variations, and PC6 explains 85% as cumulative.**



Let's first work on PCA1 and PCA2.



```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
loadings = wine.pca1$rotation
scores = wine.pca1$x
wine2 = cbind(wine, wine.pca1$x[,1:6]) 
```





```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
ggplot(wine2, aes(PC1, PC2, col=quality)) + 
  geom_point(shape = "circle", size = 1.5) +
 scale_color_viridis_c(option = "magma", direction = 1) +
 theme_minimal()
ggplot(wine2, aes(PC1, PC2, col=color, fill=color)) + 
  stat_ellipse(geom= "polygon", col="black", alpha=0.5) +
  geom_point(shape = 21, col = "black")+
   theme_minimal()
#larger alcohol and smaller density is correlated with quality
```



PCA1 and PCA2 were able to distinguish between red wines and white wines. There are some blue points lay in the red circle, that's because their chemical properties must be very close to each others. However, in general the algorithm succeed in differentiating the colors. In terms of quality, it seems the higher quality wines are the points below 0 for PCA2 and above 0 for PCA1 in general. However, let's understand how PCA1 and 2 are formed.

*Note: We have tried the other PCAs, but it seems like PCA1 and PCA2 is better at differentiating the colors and quality.*



```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
autoplot(wine.pca1, data = wine, colour = 'color', alpha = .9,
loadings = TRUE, loading.color = "red",
loadings.label = TRUE, loadings.label.size = 3)
autoplot(wine.pca1, data = wine, colour = 'quality', alpha = .9,
loadings = TRUE, loading.color = "red",
loadings.label = TRUE, loadings.label.size = 3)
```
  
From the graph above we can know that quality is negatively correlated with density, fixed acidity, chlorides. On the other hand, it is positively correlated with alcohol. 
  
  
**So we can conclude that the unsupervised algorithm used was relatively able to distinguish between red and white wines, and between their qualities. There are margin of errors, but the results still could be interpreted. **
  
  

# Question 2 - Market segmentation

  
```{r include = FALSE}
rm(list=ls())
social_marketing <- read.csv("~/Desktop/Data-Mining/ECO395M-Mining/data/social_marketing.csv", row.names = 1)
social_marketing = social_marketing[!(social_marketing$spam>=1 | social_marketing$adult>=1),]
rankings = social_marketing %>%
  colSums(c(2:37)) %>%
  as.data.frame()
rankings = rankings %>%
  arrange(desc(.))
names(rankings)[1] <- 'Category'
```
  
To analyze it, we didn't want to consider spam accounts in our analysis so we start by removing any observations that have non-zero values in the categories for adult and spam. After that we decided to see what kinds of tweets happen most ofter for the followers of the NutrientH20 brand. After ranking the tweets, we can see that chatter, photo sharing and health & nutrition are the most shared, but we should try to find common interests between followers of our account.
  
```{r echo=FALSE}
ggcorrplot::ggcorrplot(cor(social_marketing))
```
  
Using a heat map allows us to see which topic combinations are popular among followers. We want to define the market segment as common interests among users, but we also want to ensure that these common interests reflect a significant share of users. We can see that users who post about personal fitness also frequently post about health and nutrition. College and online gaming are highly correlated. This gives us an idea of which themes to present in our advertising that will peak interests.We want brand messaging to reflect the interests of our market segment. If we run PCA we can get an idea of which topics are popular within user groups, and the size of the market segment under different branding schemes. So we decided to create 6 different PCAs, because any number higher would lead to PCAs with non significant observations.
  
```{r include=FALSE}
PCAsocial = prcomp(social_marketing, rank=6, center=T)

summary(PCAsocial)
```
  
```{r echo=FALSE}
round(PCAsocial$rotation[,1:6],2) 
```
  
When we summarize the results of each PCA, we get general results of what kind of tweet would be part of each.
  
```{r include=FALSE}
loadings_summary = PCAsocial$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Social')
```
  
```{r echo=FALSE}
loadings_summary %>%
  select(Social, PC1) %>%
  arrange(desc(PC1))
```
  
Within group one we can see that health & nutrition ranks first which is right on with our imaging of the brand. This shows that this interest is the popular among this group. The second largest is personal fitness. We still need to check other groups to ensure we are capturing a significant portion of followers.
  
```{r echo=FALSE}
loadings_summary %>%
  select(Social, PC2) %>%
  arrange(desc(PC2))
```
  
Once again health & nutrition and personal fitness are the top categories. The first and second group represent ~33% of followers.
  
```{r echo=FALSE}
loadings_summary %>%
  select(Social, PC3) %>%
  arrange(desc(PC3))
```
  
This group prefers chatter and photo sharing. This indicates that an ad with a visual element could be beneficial. Interestingly they aren't positive when it comes to H&N and personal fitness.
  
```{r echo=FALSE}
loadings_summary %>%
  select(Social, PC4) %>%
  arrange(desc(PC4))
```
  
This group prefers posting about college and online gaming. While school and eco are negatively associated. H&N and personal fitness posts are not associated with this group.
  
```{r echo=FALSE}
loadings_summary %>%
  select(Social, PC5) %>%
  arrange(desc(PC5))
```
  
Health & nutrition and personal fitness are positively associated with this group. So are sports.
  
```{r echo=FALSE}
loadings_summary %>%
  select(Social, PC6) %>%
  arrange(desc(PC6))
```
  
Health & nutrition and personal fitness are positively associated with this group.  
  
After examining the groups created in our PCA we recommend positioning the product to reflect drink as a healthy option that is great for rehydrating after a workout. The company can expect this to align with roughly half of their account followers. This market segment likely participates in physical activities like working out or sports, and are looking for healthy drinks instead of sodas. The brand should position itself to be the premier choice for this market segment.  
  
# Question 3 - Finish Graph separately
```{r include=FALSE}
rm(list=ls())

groceries <- read.delim("~/Desktop/Data-Mining/ECO395M-Mining/data/groceries.txt", header=FALSE, row.names=NULL)

groceries$person <- 1:nrow(groceries)

clean = groceries %>%
  mutate(V1 = strsplit(as.character(V1), ",")) %>% 
  unnest(V1)

grocerie_list = split(x=clean$V1, f=clean$person)

grocerie_list = lapply(grocerie_list, unique)

purchases = as(grocerie_list, "transactions")

grocerelations = apriori(purchases, 
                     parameter=list(support=.01, confidence=.05, maxlen=4))

inspect(grocerelations)

plot(grocerelations)

sub1 = subset(grocerelations, subset=confidence > 0.05 & support > 0.01  & lift > 2)
saveAsGraph(sub1, file = "grocerelations.graphml")
```
  
This dataset was composed of the basket of goods of 9835 customers. To create the network we decided to use a confidence of at least 5 percent, support of at least 1 percent and max lenght of 4. We wanted to see what products were purchased at least 5 percent in relation to other product and 1 percent by itself and see a maximum of four items that this good relates to. However, we found many rules that were not very significant, so when we graph it, we decided to maintain only rules that double the chance of purchasing the second item when the first was already on the basket.


```{r echo=FALSE}
knitr::include_graphics("HM4_files/figure-markdown_strict/figureq3.png")
```
  
In this graph we see that there are less rules that double the purchase probability. We can see some meaninful relation between yougurt and berries. But we also see some weird relations, as whipped cream and other vegetables.  
  
```{r echo=FALSE}
knitr::include_graphics("HM4_files/figure-markdown_strict/figureq3a.png")
```
  
To see some relations better, we decided to check two nodes, which were "other vegetables" and "root vegetables". We can see that "other vegetables" not only relate to other products of their cluster, but also to many different products varying from all clusters. One can argue that "other vegetables" are purchased by majority of the customers in our dataset and that they have increased probability of being purchased in relation to a phetora of items.  
  
```{r echo=FALSE}
knitr::include_graphics("HM4_files/figure-markdown_strict/figureq3b.png")
```
  
As for the "root vegetables", we see the relation whithin it's own cluster, but we see some relation with other clusters, but not as many as the previous node. We can see in this node one problem from this dataset, which is that it separates some items into multiple categories. "Onions" are an item classified in the "other vegetable", but they are also "root vegetables", so we would expect them to be closely related. Using this network, a supermarket has two options, it can keep related items located closely to each other, so that the customer will acquire both, or knowing that some have high purchase relation, it can keep them apart, which will make the customer transite through the store for longer and possible purchase other items with smaller confidence.  



